{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='GeForce GTX 1080', major=6, minor=1, total_memory=8192MB, multi_processor_count=20)\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and module import\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False # if True, uses existing mini_train_encoded.csv file so this runs fast\n",
    "save_data = False # default: False. This is for diagnostic purposes.  Remove later.\n",
    "\n",
    "# parameters for NN training\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(torch.cuda.get_device_properties(device))\n",
    "    chunk_size = 1000000 if not debug else 5 # run in meaningful chunks so that GPU doesn't run out of memory\n",
    "    epochs = 300 # can do this in 90 min\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"running on CPU\")\n",
    "    chunk_size = 10000000 # load all the data at once\n",
    "    epochs = 100 # run shorter because it takes a lot longer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.batch_norm_num = nn.BatchNorm1d(numerical_cols).to(device)\n",
    "        all_layers = []\n",
    "        input_size = numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "        \n",
    "    def forward(self, x_data):\n",
    "        x_data = self.batch_norm_num(x_data)\n",
    "        x = torch.cat([x_data], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tain data: 6245038 rows, 150 columns\n"
     ]
    }
   ],
   "source": [
    "# Import DF from CSV\n",
    "\n",
    "if debug == True:\n",
    "    dataset = 'data/mini_train_encoded.csv'\n",
    "else:\n",
    "    dataset = 'data/train_encoded.csv'\n",
    "\n",
    "df = pd.read_csv(dataset)\n",
    "\n",
    "# Remove hasdetections from the df.  Make it a new df for labels\n",
    "labels = df['HasDetections'].to_numpy()\n",
    "df.drop(columns=['HasDetections'], inplace=True)\n",
    "\n",
    "# Iterate and convert all data values\n",
    "\n",
    "cols = []\n",
    "for c in df.columns:\n",
    "    df[c] = df[c].astype(np.float64) # Convert dtypes to be all identical for pytorch\n",
    "\n",
    "print(f\"Tain data: {len(df)} rows, {len(df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details:\n",
      "Model(\n",
      "  (batch_norm_num): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=150, out_features=180, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=180, out_features=50, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.4, inplace=False)\n",
      "    (12): Linear(in_features=50, out_features=20, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.4, inplace=False)\n",
      "    (16): Linear(in_features=20, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(len(df.columns), 2, [180,50,50,20], p=0.4).to(device)\n",
    "print(\"Model details:\")\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Using CrossEntropyLoss because we effectively have an unbalanced training set\n",
    "# In other words, not all inputs are normalized\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NN training\n",
      "....... Epoch:   1, loss: 0.799, elapsed:    18 s\n",
      "....... Epoch:   2, loss: 0.780, elapsed:    35 s\n",
      "....... Epoch:   3, loss: 0.768, elapsed:    53 s\n",
      "....... Epoch:   4, loss: 0.757, elapsed:    71 s\n",
      "....... Epoch:   5, loss: 0.751, elapsed:    88 s\n",
      "....... Epoch:   6, loss: 0.742, elapsed:   106 s\n",
      "....... Epoch:   7, loss: 0.738, elapsed:   123 s\n",
      "....... Epoch:   8, loss: 0.732, elapsed:   141 s\n",
      "....... Epoch:   9, loss: 0.728, elapsed:   159 s\n",
      "....... Epoch:  10, loss: 0.721, elapsed:   177 s\n",
      "....... Epoch:  11, loss: 0.717, elapsed:   195 s\n",
      "....... Epoch:  12, loss: 0.713, elapsed:   212 s\n",
      "....... Epoch:  13, loss: 0.708, elapsed:   231 s\n",
      "....... Epoch:  14, loss: 0.703, elapsed:   249 s\n",
      "....... Epoch:  15, loss: 0.703, elapsed:   267 s\n",
      "....... Epoch:  16, loss: 0.698, elapsed:   286 s\n",
      "....... Epoch:  17, loss: 0.696, elapsed:   304 s\n",
      "....... Epoch:  18, loss: 0.693, elapsed:   322 s\n",
      "....... Epoch:  19, loss: 0.690, elapsed:   340 s\n",
      "....... Epoch:  20, loss: 0.687, elapsed:   358 s\n",
      "....... Epoch:  21, loss: 0.685, elapsed:   376 s\n",
      "....... Epoch:  22, loss: 0.683, elapsed:   394 s\n",
      "....... Epoch:  23, loss: 0.680, elapsed:   412 s\n",
      "....... Epoch:  24, loss: 0.680, elapsed:   431 s\n",
      "....... Epoch:  25, loss: 0.677, elapsed:   449 s\n",
      "....... Epoch:  26, loss: 0.677, elapsed:   467 s\n",
      "....... Epoch:  27, loss: 0.674, elapsed:   485 s\n",
      "....... Epoch:  28, loss: 0.675, elapsed:   503 s\n",
      "....... Epoch:  29, loss: 0.671, elapsed:   521 s\n",
      "....... Epoch:  30, loss: 0.669, elapsed:   539 s\n",
      "....... Epoch:  31, loss: 0.669, elapsed:   557 s\n",
      "....... Epoch:  32, loss: 0.668, elapsed:   575 s\n",
      "....... Epoch:  33, loss: 0.667, elapsed:   593 s\n",
      "....... Epoch:  34, loss: 0.665, elapsed:   611 s\n",
      "....... Epoch:  35, loss: 0.664, elapsed:   629 s\n",
      "....... Epoch:  36, loss: 0.664, elapsed:   647 s\n",
      "....... Epoch:  37, loss: 0.664, elapsed:   664 s\n",
      "....... Epoch:  38, loss: 0.663, elapsed:   682 s\n",
      "....... Epoch:  39, loss: 0.662, elapsed:   700 s\n",
      "....... Epoch:  40, loss: 0.661, elapsed:   717 s\n",
      "....... Epoch:  41, loss: 0.659, elapsed:   735 s\n",
      "....... Epoch:  42, loss: 0.660, elapsed:   753 s\n",
      "....... Epoch:  43, loss: 0.658, elapsed:   771 s\n",
      "....... Epoch:  44, loss: 0.658, elapsed:   788 s\n",
      "....... Epoch:  45, loss: 0.658, elapsed:   806 s\n",
      "....... Epoch:  46, loss: 0.657, elapsed:   824 s\n",
      "....... Epoch:  47, loss: 0.657, elapsed:   841 s\n",
      "....... Epoch:  48, loss: 0.656, elapsed:   859 s\n",
      "....... Epoch:  49, loss: 0.654, elapsed:   877 s\n",
      "....... Epoch:  50, loss: 0.655, elapsed:   894 s\n",
      "....... Epoch:  51, loss: 0.655, elapsed:   912 s\n",
      "....... Epoch:  52, loss: 0.653, elapsed:   929 s\n",
      "....... Epoch:  53, loss: 0.655, elapsed:   947 s\n",
      "....... Epoch:  54, loss: 0.652, elapsed:   965 s\n",
      "....... Epoch:  55, loss: 0.652, elapsed:   983 s\n",
      "....... Epoch:  56, loss: 0.651, elapsed:  1000 s\n",
      "....... Epoch:  57, loss: 0.652, elapsed:  1018 s\n",
      "....... Epoch:  58, loss: 0.652, elapsed:  1036 s\n",
      "....... Epoch:  59, loss: 0.652, elapsed:  1054 s\n",
      "....... Epoch:  60, loss: 0.652, elapsed:  1071 s\n",
      "....... Epoch:  61, loss: 0.650, elapsed:  1089 s\n",
      "....... Epoch:  62, loss: 0.650, elapsed:  1107 s\n",
      "....... Epoch:  63, loss: 0.649, elapsed:  1125 s\n",
      "....... Epoch:  64, loss: 0.649, elapsed:  1142 s\n",
      "....... Epoch:  65, loss: 0.651, elapsed:  1160 s\n",
      "....... Epoch:  66, loss: 0.650, elapsed:  1178 s\n",
      "....... Epoch:  67, loss: 0.650, elapsed:  1196 s\n",
      "....... Epoch:  68, loss: 0.650, elapsed:  1214 s\n",
      "....... Epoch:  69, loss: 0.648, elapsed:  1231 s\n",
      "....... Epoch:  70, loss: 0.648, elapsed:  1249 s\n",
      "....... Epoch:  71, loss: 0.648, elapsed:  1267 s\n",
      "....... Epoch:  72, loss: 0.647, elapsed:  1285 s\n",
      "....... Epoch:  73, loss: 0.649, elapsed:  1304 s\n",
      "....... Epoch:  74, loss: 0.647, elapsed:  1322 s\n",
      "....... Epoch:  75, loss: 0.647, elapsed:  1340 s\n",
      "....... Epoch:  76, loss: 0.647, elapsed:  1358 s\n",
      "....... Epoch:  77, loss: 0.648, elapsed:  1376 s\n",
      "....... Epoch:  78, loss: 0.646, elapsed:  1394 s\n",
      "....... Epoch:  79, loss: 0.645, elapsed:  1412 s\n",
      "....... Epoch:  80, loss: 0.646, elapsed:  1429 s\n",
      "....... Epoch:  81, loss: 0.647, elapsed:  1447 s\n",
      "....... Epoch:  82, loss: 0.645, elapsed:  1465 s\n",
      "....... Epoch:  83, loss: 0.645, elapsed:  1483 s\n",
      "....... Epoch:  84, loss: 0.646, elapsed:  1501 s\n",
      "....... Epoch:  85, loss: 0.644, elapsed:  1519 s\n",
      "....... Epoch:  86, loss: 0.645, elapsed:  1537 s\n",
      "....... Epoch:  87, loss: 0.646, elapsed:  1555 s\n",
      "....... Epoch:  88, loss: 0.645, elapsed:  1572 s\n",
      "....... Epoch:  89, loss: 0.644, elapsed:  1590 s\n",
      "....... Epoch:  90, loss: 0.643, elapsed:  1609 s\n",
      "....... Epoch:  91, loss: 0.643, elapsed:  1626 s\n",
      "....... Epoch:  92, loss: 0.643, elapsed:  1644 s\n",
      "....... Epoch:  93, loss: 0.644, elapsed:  1662 s\n",
      "."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ed7a40f60713>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# grab columns and labels, push into device (possibly GPU)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_chunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[0msl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[0mexpanded_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# train\n",
    "#\n",
    "\n",
    "print(\"Starting NN training\")\n",
    "start = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    rows_left = len(df)\n",
    "    current = 0\n",
    "    chunk_num = math.ceil(rows_left/chunk_size)\n",
    "\n",
    "    #\n",
    "    # do every epoch in chunks\n",
    "    #\n",
    "    \n",
    "    model.zero_grad()\n",
    "    while rows_left > 0:\n",
    "        rows = (chunk_size if rows_left >= chunk_size else rows_left)\n",
    "        chunk = df[current:current+rows]\n",
    "        label_chunk = labels[current:current+rows]\n",
    "        \n",
    "        current += rows\n",
    "        rows_left -= rows\n",
    "        \n",
    "        print(\".\", end=\"\")\n",
    "        \n",
    "        # grab columns and labels, push into device (possibly GPU)\n",
    "        cols = [chunk[col].values for col in chunk.columns]\n",
    "        data = np.stack(cols, 1)\n",
    "        data = torch.tensor(data, dtype=torch.float).to(device)\n",
    "        output = torch.tensor(label_chunk).to(device)\n",
    "        \n",
    "        prediction = model(data)\n",
    "        single_loss = loss_function(prediction, output)\n",
    "        single_loss /= chunk_num\n",
    "        single_loss.backward()\n",
    "\n",
    "    # after done with all chunks, process the step and reset the gradients\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "      \n",
    "    print(f' Epoch: {i+1:3}, loss: {single_loss.item()*chunk_num:1.3f},', end=' ')\n",
    "    print(f'elapsed: {time.time()-start:5.0f} s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN training complete. Minutes elapsed: 28\n"
     ]
    }
   ],
   "source": [
    "print(f\"NN training complete. Minutes elapsed: {(time.time()-start)/60:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (batch_norm_num): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=150, out_features=180, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=180, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.4, inplace=False)\n",
       "    (12): Linear(in_features=50, out_features=20, bias=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Dropout(p=0.4, inplace=False)\n",
       "    (16): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Phase two - model evaluation\n",
    "\n",
    "model.eval() # switch into eval mode\n",
    "model.cpu() # get model back into CPU space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: 1338222 rows, 150 columns\n"
     ]
    }
   ],
   "source": [
    "# Import dev DF from CSV\n",
    "\n",
    "if debug == True:\n",
    "    dataset = 'data/mini_dev_encoded.csv'\n",
    "else:\n",
    "    dataset = 'data/dev_encoded.csv'\n",
    "\n",
    "test = pd.read_csv(dataset)\n",
    "\n",
    "if \"HasDetections\" in test.columns:\n",
    "    # Remove hasdetections from the df.  Make it a new df for labels\n",
    "    test_labels = test['HasDetections'].to_numpy()\n",
    "    test.drop(columns=['HasDetections'], inplace=True)\n",
    "\n",
    "# Iterate and convert all data values\n",
    "\n",
    "cols = []\n",
    "for c in test.columns:\n",
    "    test[c] = test[c].astype(np.float64) # Convert dtypes to be all identical for pytorch\n",
    "\n",
    "print(f\"Test data: {len(test)} rows, {len(test.columns)} columns\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1338222, 150])\n",
      "torch.Size([1338222, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# grab columns and labels, push into device (possibly GPU)\n",
    "cols = [test[col].values for col in test.columns]\n",
    "data = np.stack(cols, 1)\n",
    "data = torch.tensor(data, dtype=torch.float)\n",
    "print(data.shape)\n",
    "\n",
    "output = model(data)\n",
    "print(output.shape)\n",
    "predictions = np.argmax(output.cpu().data.numpy(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1338222,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1338222,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6347571628623652\n",
      "[1 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1]\n",
      "[0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=test_labels, y_pred=predictions))\n",
    "print(predictions[:30])\n",
    "print(labels[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
