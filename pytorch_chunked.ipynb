{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup and module import\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='GeForce GTX 1080', major=6, minor=1, total_memory=8192MB, multi_processor_count=20)\n"
     ]
    }
   ],
   "source": [
    "debug = False # if True, uses existing mini_train_encoded.csv file so this runs fast\n",
    "save_data = False # default: False. This is for diagnostic purposes.  Remove later.\n",
    "\n",
    "# parameters for NN training\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(torch.cuda.get_device_properties(device))\n",
    "    chunk_size = 1000000 if not debug else 5 # run in meaningful chunks so that GPU doesn't run out of memory\n",
    "    epochs = 300 # can do this in 90 min\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"running on CPU\")\n",
    "    chunk_size = 10000000 # load all the data at once\n",
    "    epochs = 100 # run shorter because it takes a lot longer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        self.batch_norm_num = nn.BatchNorm1d(numerical_cols).to(device)\n",
    "        all_layers = []\n",
    "        input_size = numerical_cols\n",
    "\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "        \n",
    "    def forward(self, x_data):\n",
    "        x_data = self.batch_norm_num(x_data)\n",
    "        x = torch.cat([x_data], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tain data: 6245038 rows, 150 columns\n"
     ]
    }
   ],
   "source": [
    "# Import DF from CSV\n",
    "\n",
    "if debug == True:\n",
    "    dataset = 'data/mini_train_encoded.csv'\n",
    "else:\n",
    "    dataset = 'data/train_encoded.csv'\n",
    "\n",
    "df = pd.read_csv(dataset)\n",
    "\n",
    "# Remove hasdetections from the df.  Make it a new df for labels\n",
    "labels = df['HasDetections'].to_numpy()\n",
    "df.drop(columns=['HasDetections'], inplace=True)\n",
    "\n",
    "# Iterate and convert all data values\n",
    "\n",
    "cols = []\n",
    "for c in df.columns:\n",
    "    df[c] = df[c].astype(np.float64) # Convert dtypes to be all identical for pytorch\n",
    "\n",
    "print(f\"Tain data: {len(df)} rows, {len(df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model details:\n",
      "Model(\n",
      "  (batch_norm_num): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=150, out_features=180, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=180, out_features=50, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.4, inplace=False)\n",
      "    (12): Linear(in_features=50, out_features=20, bias=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.4, inplace=False)\n",
      "    (16): Linear(in_features=20, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(len(df.columns), 2, [180,50,50,20], p=0.4).to(device)\n",
    "print(\"Model details:\")\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Using CrossEntropyLoss because we effectively have an unbalanced training set\n",
    "# In other words, not all inputs are normalized\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NN training\n",
      "....... Epoch:   1, loss: 0.782, elapsed:    18 s\n",
      "....... Epoch:   2, loss: 0.761, elapsed:    36 s\n",
      "....... Epoch:   3, loss: 0.749, elapsed:    54 s\n",
      "....... Epoch:   4, loss: 0.736, elapsed:    72 s\n",
      "....... Epoch:   5, loss: 0.729, elapsed:    90 s\n",
      "....... Epoch:   6, loss: 0.723, elapsed:   108 s\n",
      "....... Epoch:   7, loss: 0.717, elapsed:   125 s\n",
      "....... Epoch:   8, loss: 0.712, elapsed:   143 s\n",
      "....... Epoch:   9, loss: 0.708, elapsed:   161 s\n",
      "....... Epoch:  10, loss: 0.704, elapsed:   179 s\n",
      "....... Epoch:  11, loss: 0.703, elapsed:   197 s\n",
      "....... Epoch:  12, loss: 0.698, elapsed:   215 s\n",
      "....... Epoch:  13, loss: 0.698, elapsed:   233 s\n",
      "....... Epoch:  14, loss: 0.693, elapsed:   251 s\n",
      "....... Epoch:  15, loss: 0.690, elapsed:   269 s\n",
      "....... Epoch:  16, loss: 0.690, elapsed:   287 s\n",
      "....... Epoch:  17, loss: 0.685, elapsed:   305 s\n",
      "....... Epoch:  18, loss: 0.683, elapsed:   323 s\n",
      "....... Epoch:  19, loss: 0.679, elapsed:   340 s\n",
      "....... Epoch:  20, loss: 0.681, elapsed:   358 s\n",
      "....... Epoch:  21, loss: 0.678, elapsed:   376 s\n",
      "....... Epoch:  22, loss: 0.675, elapsed:   394 s\n",
      "....... Epoch:  23, loss: 0.674, elapsed:   412 s\n",
      "....... Epoch:  24, loss: 0.673, elapsed:   430 s\n",
      "....... Epoch:  25, loss: 0.671, elapsed:   447 s\n",
      "....... Epoch:  26, loss: 0.669, elapsed:   465 s\n",
      "....... Epoch:  27, loss: 0.668, elapsed:   483 s\n",
      "....... Epoch:  28, loss: 0.667, elapsed:   501 s\n",
      "....... Epoch:  29, loss: 0.666, elapsed:   519 s\n",
      "....... Epoch:  30, loss: 0.664, elapsed:   537 s\n",
      "....... Epoch:  31, loss: 0.662, elapsed:   555 s\n",
      "....... Epoch:  32, loss: 0.663, elapsed:   574 s\n",
      "....... Epoch:  33, loss: 0.660, elapsed:   591 s\n",
      "....... Epoch:  34, loss: 0.658, elapsed:   610 s\n",
      "....... Epoch:  35, loss: 0.660, elapsed:   627 s\n",
      "....... Epoch:  36, loss: 0.658, elapsed:   645 s\n",
      "....... Epoch:  37, loss: 0.657, elapsed:   663 s\n",
      "....... Epoch:  38, loss: 0.659, elapsed:   681 s\n",
      "....... Epoch:  39, loss: 0.656, elapsed:   699 s\n",
      "....... Epoch:  40, loss: 0.657, elapsed:   716 s\n",
      "....... Epoch:  41, loss: 0.656, elapsed:   734 s\n",
      "....... Epoch:  42, loss: 0.655, elapsed:   752 s\n",
      "....... Epoch:  43, loss: 0.656, elapsed:   770 s\n",
      "....... Epoch:  44, loss: 0.654, elapsed:   788 s\n",
      "....... Epoch:  45, loss: 0.656, elapsed:   806 s\n",
      "....... Epoch:  46, loss: 0.653, elapsed:   823 s\n",
      "....... Epoch:  47, loss: 0.653, elapsed:   841 s\n",
      "....... Epoch:  48, loss: 0.654, elapsed:   859 s\n",
      "....... Epoch:  49, loss: 0.654, elapsed:   877 s\n",
      "....... Epoch:  50, loss: 0.651, elapsed:   895 s\n",
      "....... Epoch:  51, loss: 0.652, elapsed:   913 s\n",
      "....... Epoch:  52, loss: 0.652, elapsed:   930 s\n",
      "....... Epoch:  53, loss: 0.650, elapsed:   948 s\n",
      "....... Epoch:  54, loss: 0.651, elapsed:   966 s\n",
      "....... Epoch:  55, loss: 0.650, elapsed:   984 s\n",
      "....... Epoch:  56, loss: 0.650, elapsed:  1002 s\n",
      "....... Epoch:  57, loss: 0.649, elapsed:  1020 s\n",
      "....... Epoch:  58, loss: 0.649, elapsed:  1038 s\n",
      "....... Epoch:  59, loss: 0.649, elapsed:  1055 s\n",
      "....... Epoch:  60, loss: 0.648, elapsed:  1073 s\n",
      "....... Epoch:  61, loss: 0.649, elapsed:  1091 s\n",
      "....... Epoch:  62, loss: 0.649, elapsed:  1109 s\n",
      "....... Epoch:  63, loss: 0.649, elapsed:  1127 s\n",
      "....... Epoch:  64, loss: 0.648, elapsed:  1144 s\n",
      "....... Epoch:  65, loss: 0.649, elapsed:  1162 s\n",
      "....... Epoch:  66, loss: 0.648, elapsed:  1180 s\n",
      "....... Epoch:  67, loss: 0.648, elapsed:  1198 s\n",
      "....... Epoch:  68, loss: 0.649, elapsed:  1215 s\n",
      "....... Epoch:  69, loss: 0.646, elapsed:  1233 s\n",
      "....... Epoch:  70, loss: 0.646, elapsed:  1251 s\n",
      "....... Epoch:  71, loss: 0.646, elapsed:  1269 s\n",
      "....... Epoch:  72, loss: 0.646, elapsed:  1287 s\n",
      "....... Epoch:  73, loss: 0.646, elapsed:  1304 s\n",
      "....... Epoch:  74, loss: 0.646, elapsed:  1322 s\n",
      "....... Epoch:  75, loss: 0.645, elapsed:  1340 s\n",
      "....... Epoch:  76, loss: 0.645, elapsed:  1358 s\n",
      "....... Epoch:  77, loss: 0.645, elapsed:  1376 s\n",
      "....... Epoch:  78, loss: 0.645, elapsed:  1393 s\n",
      "....... Epoch:  79, loss: 0.645, elapsed:  1411 s\n",
      "....... Epoch:  80, loss: 0.644, elapsed:  1429 s\n",
      "....... Epoch:  81, loss: 0.644, elapsed:  1447 s\n",
      "....... Epoch:  82, loss: 0.644, elapsed:  1465 s\n",
      "....... Epoch:  83, loss: 0.643, elapsed:  1482 s\n",
      "....... Epoch:  84, loss: 0.644, elapsed:  1500 s\n",
      "....... Epoch:  85, loss: 0.644, elapsed:  1518 s\n",
      "....... Epoch:  86, loss: 0.643, elapsed:  1536 s\n",
      "....... Epoch:  87, loss: 0.643, elapsed:  1554 s\n",
      "....... Epoch:  88, loss: 0.645, elapsed:  1571 s\n",
      "....... Epoch:  89, loss: 0.643, elapsed:  1589 s\n",
      "....... Epoch:  90, loss: 0.642, elapsed:  1607 s\n",
      "....... Epoch:  91, loss: 0.642, elapsed:  1625 s\n",
      "....... Epoch:  92, loss: 0.644, elapsed:  1643 s\n",
      "....... Epoch:  93, loss: 0.643, elapsed:  1660 s\n",
      "....... Epoch:  94, loss: 0.643, elapsed:  1678 s\n",
      "....... Epoch:  95, loss: 0.642, elapsed:  1696 s\n",
      "....... Epoch:  96, loss: 0.644, elapsed:  1714 s\n",
      "....... Epoch:  97, loss: 0.642, elapsed:  1732 s\n",
      "....... Epoch:  98, loss: 0.643, elapsed:  1749 s\n",
      "....... Epoch:  99, loss: 0.641, elapsed:  1767 s\n",
      "....... Epoch: 100, loss: 0.642, elapsed:  1785 s\n",
      "....... Epoch: 101, loss: 0.641, elapsed:  1803 s\n",
      "....... Epoch: 102, loss: 0.642, elapsed:  1821 s\n",
      "....... Epoch: 103, loss: 0.641, elapsed:  1839 s\n",
      "....... Epoch: 104, loss: 0.643, elapsed:  1856 s\n",
      "....... Epoch: 105, loss: 0.641, elapsed:  1874 s\n",
      "....... Epoch: 106, loss: 0.640, elapsed:  1892 s\n",
      "....... Epoch: 107, loss: 0.641, elapsed:  1910 s\n",
      "....... Epoch: 108, loss: 0.641, elapsed:  1928 s\n",
      "....... Epoch: 109, loss: 0.641, elapsed:  1945 s\n",
      "....... Epoch: 110, loss: 0.640, elapsed:  1963 s\n",
      "....... Epoch: 111, loss: 0.640, elapsed:  1981 s\n",
      "....... Epoch: 112, loss: 0.640, elapsed:  1999 s\n",
      "....... Epoch: 113, loss: 0.640, elapsed:  2017 s\n",
      "....... Epoch: 114, loss: 0.640, elapsed:  2035 s\n",
      "....... Epoch: 115, loss: 0.640, elapsed:  2052 s\n",
      "....... Epoch: 116, loss: 0.640, elapsed:  2070 s\n",
      "....... Epoch: 117, loss: 0.639, elapsed:  2088 s\n",
      "....... Epoch: 118, loss: 0.639, elapsed:  2105 s\n",
      "....... Epoch: 119, loss: 0.639, elapsed:  2123 s\n",
      "....... Epoch: 120, loss: 0.639, elapsed:  2141 s\n",
      "....... Epoch: 121, loss: 0.639, elapsed:  2159 s\n",
      "....... Epoch: 122, loss: 0.639, elapsed:  2176 s\n",
      "....... Epoch: 123, loss: 0.639, elapsed:  2194 s\n",
      "....... Epoch: 124, loss: 0.638, elapsed:  2212 s\n",
      "....... Epoch: 125, loss: 0.638, elapsed:  2229 s\n",
      "....... Epoch: 126, loss: 0.638, elapsed:  2247 s\n",
      "....... Epoch: 127, loss: 0.639, elapsed:  2266 s\n",
      "....... Epoch: 128, loss: 0.639, elapsed:  2283 s\n",
      "....... Epoch: 129, loss: 0.638, elapsed:  2301 s\n",
      "....... Epoch: 130, loss: 0.637, elapsed:  2319 s\n",
      "....... Epoch: 131, loss: 0.637, elapsed:  2337 s\n",
      "....... Epoch: 132, loss: 0.638, elapsed:  2354 s\n",
      "....... Epoch: 133, loss: 0.637, elapsed:  2372 s\n",
      "....... Epoch: 134, loss: 0.638, elapsed:  2390 s\n",
      "....... Epoch: 135, loss: 0.638, elapsed:  2407 s\n",
      "....... Epoch: 136, loss: 0.638, elapsed:  2425 s\n",
      "....... Epoch: 137, loss: 0.637, elapsed:  2443 s\n",
      "....... Epoch: 138, loss: 0.637, elapsed:  2461 s\n",
      "....... Epoch: 139, loss: 0.637, elapsed:  2478 s\n",
      "....... Epoch: 140, loss: 0.637, elapsed:  2496 s\n",
      "....... Epoch: 141, loss: 0.636, elapsed:  2514 s\n",
      "....... Epoch: 142, loss: 0.637, elapsed:  2531 s\n",
      "....... Epoch: 143, loss: 0.637, elapsed:  2549 s\n",
      "....... Epoch: 144, loss: 0.636, elapsed:  2567 s\n",
      "....... Epoch: 145, loss: 0.636, elapsed:  2584 s\n",
      "....... Epoch: 146, loss: 0.636, elapsed:  2602 s\n",
      "....... Epoch: 147, loss: 0.636, elapsed:  2620 s\n",
      "....... Epoch: 148, loss: 0.636, elapsed:  2638 s\n",
      "....... Epoch: 149, loss: 0.637, elapsed:  2655 s\n",
      "....... Epoch: 150, loss: 0.635, elapsed:  2673 s\n",
      "....... Epoch: 151, loss: 0.636, elapsed:  2691 s\n",
      "....... Epoch: 152, loss: 0.636, elapsed:  2708 s\n",
      "....... Epoch: 153, loss: 0.636, elapsed:  2726 s\n",
      "....... Epoch: 154, loss: 0.636, elapsed:  2744 s\n",
      "....... Epoch: 155, loss: 0.635, elapsed:  2762 s\n",
      "....... Epoch: 156, loss: 0.635, elapsed:  2780 s\n",
      "....... Epoch: 157, loss: 0.636, elapsed:  2797 s\n",
      "....... Epoch: 158, loss: 0.636, elapsed:  2815 s\n",
      "....... Epoch: 159, loss: 0.635, elapsed:  2833 s\n",
      "....... Epoch: 160, loss: 0.635, elapsed:  2850 s\n",
      "....... Epoch: 161, loss: 0.635, elapsed:  2868 s\n",
      "....... Epoch: 162, loss: 0.635, elapsed:  2886 s\n",
      "....... Epoch: 163, loss: 0.636, elapsed:  2904 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....... Epoch: 164, loss: 0.634, elapsed:  2922 s\n",
      "....... Epoch: 165, loss: 0.634, elapsed:  2939 s\n",
      "....... Epoch: 166, loss: 0.635, elapsed:  2957 s\n",
      "....... Epoch: 167, loss: 0.635, elapsed:  2975 s\n",
      "....... Epoch: 168, loss: 0.634, elapsed:  2992 s\n",
      "....... Epoch: 169, loss: 0.634, elapsed:  3010 s\n",
      "....... Epoch: 170, loss: 0.634, elapsed:  3028 s\n",
      "....... Epoch: 171, loss: 0.635, elapsed:  3046 s\n",
      "....... Epoch: 172, loss: 0.634, elapsed:  3063 s\n",
      "....... Epoch: 173, loss: 0.634, elapsed:  3081 s\n",
      "....... Epoch: 174, loss: 0.634, elapsed:  3099 s\n",
      "....... Epoch: 175, loss: 0.634, elapsed:  3116 s\n",
      "....... Epoch: 176, loss: 0.633, elapsed:  3134 s\n",
      "....... Epoch: 177, loss: 0.634, elapsed:  3152 s\n",
      "....... Epoch: 178, loss: 0.634, elapsed:  3170 s\n",
      "....... Epoch: 179, loss: 0.633, elapsed:  3187 s\n",
      "....... Epoch: 180, loss: 0.634, elapsed:  3205 s\n",
      "....... Epoch: 181, loss: 0.634, elapsed:  3223 s\n",
      "....... Epoch: 182, loss: 0.634, elapsed:  3240 s\n",
      "....... Epoch: 183, loss: 0.633, elapsed:  3258 s\n",
      "....... Epoch: 184, loss: 0.633, elapsed:  3276 s\n",
      "....... Epoch: 185, loss: 0.633, elapsed:  3293 s\n",
      "....... Epoch: 186, loss: 0.633, elapsed:  3311 s\n",
      "....... Epoch: 187, loss: 0.633, elapsed:  3329 s\n",
      "....... Epoch: 188, loss: 0.633, elapsed:  3347 s\n",
      "....... Epoch: 189, loss: 0.632, elapsed:  3364 s\n",
      "....... Epoch: 190, loss: 0.633, elapsed:  3382 s\n",
      "....... Epoch: 191, loss: 0.632, elapsed:  3400 s\n",
      "....... Epoch: 192, loss: 0.632, elapsed:  3417 s\n",
      "....... Epoch: 193, loss: 0.633, elapsed:  3435 s\n",
      "....... Epoch: 194, loss: 0.633, elapsed:  3453 s\n",
      "....... Epoch: 195, loss: 0.632, elapsed:  3470 s\n",
      "....... Epoch: 196, loss: 0.632, elapsed:  3488 s\n",
      "....... Epoch: 197, loss: 0.632, elapsed:  3506 s\n",
      "....... Epoch: 198, loss: 0.632, elapsed:  3523 s\n",
      "....... Epoch: 199, loss: 0.632, elapsed:  3541 s\n",
      "....... Epoch: 200, loss: 0.632, elapsed:  3559 s\n",
      "....... Epoch: 201, loss: 0.632, elapsed:  3576 s\n",
      "....... Epoch: 202, loss: 0.632, elapsed:  3594 s\n",
      "....... Epoch: 203, loss: 0.632, elapsed:  3612 s\n",
      "....... Epoch: 204, loss: 0.631, elapsed:  3630 s\n",
      "....... Epoch: 205, loss: 0.632, elapsed:  3647 s\n",
      "....... Epoch: 206, loss: 0.631, elapsed:  3665 s\n",
      "....... Epoch: 207, loss: 0.631, elapsed:  3686 s\n",
      "....... Epoch: 208, loss: 0.631, elapsed:  3711 s\n",
      "....... Epoch: 209, loss: 0.633, elapsed:  3736 s\n",
      "....... Epoch: 210, loss: 0.632, elapsed:  3761 s\n",
      "....... Epoch: 211, loss: 0.631, elapsed:  3784 s\n",
      "....... Epoch: 212, loss: 0.632, elapsed:  3808 s\n",
      "....... Epoch: 213, loss: 0.631, elapsed:  3831 s\n",
      "....... Epoch: 214, loss: 0.631, elapsed:  3854 s\n",
      "....... Epoch: 215, loss: 0.632, elapsed:  3878 s\n",
      "....... Epoch: 216, loss: 0.631, elapsed:  3902 s\n",
      "....... Epoch: 217, loss: 0.631, elapsed:  3920 s\n",
      "....... Epoch: 218, loss: 0.631, elapsed:  3937 s\n",
      "....... Epoch: 219, loss: 0.631, elapsed:  3955 s\n",
      "....... Epoch: 220, loss: 0.631, elapsed:  3973 s\n",
      "....... Epoch: 221, loss: 0.631, elapsed:  3990 s\n",
      "....... Epoch: 222, loss: 0.630, elapsed:  4008 s\n",
      "....... Epoch: 223, loss: 0.631, elapsed:  4026 s\n",
      "....... Epoch: 224, loss: 0.631, elapsed:  4044 s\n",
      "....... Epoch: 225, loss: 0.631, elapsed:  4062 s\n",
      "....... Epoch: 226, loss: 0.631, elapsed:  4080 s\n",
      "....... Epoch: 227, loss: 0.631, elapsed:  4098 s\n",
      "....... Epoch: 228, loss: 0.631, elapsed:  4116 s\n",
      "....... Epoch: 229, loss: 0.630, elapsed:  4133 s\n",
      "....... Epoch: 230, loss: 0.630, elapsed:  4151 s\n",
      "....... Epoch: 231, loss: 0.631, elapsed:  4169 s\n",
      "....... Epoch: 232, loss: 0.630, elapsed:  4187 s\n",
      "....... Epoch: 233, loss: 0.630, elapsed:  4204 s\n",
      "....... Epoch: 234, loss: 0.630, elapsed:  4222 s\n",
      "....... Epoch: 235, loss: 0.630, elapsed:  4240 s\n",
      "....... Epoch: 236, loss: 0.630, elapsed:  4258 s\n",
      "....... Epoch: 237, loss: 0.630, elapsed:  4275 s\n",
      "....... Epoch: 238, loss: 0.630, elapsed:  4293 s\n",
      "....... Epoch: 239, loss: 0.630, elapsed:  4311 s\n",
      "....... Epoch: 240, loss: 0.629, elapsed:  4329 s\n",
      "....... Epoch: 241, loss: 0.630, elapsed:  4346 s\n",
      "....... Epoch: 242, loss: 0.630, elapsed:  4364 s\n",
      "....... Epoch: 243, loss: 0.631, elapsed:  4382 s\n",
      "....... Epoch: 244, loss: 0.630, elapsed:  4399 s\n",
      "....... Epoch: 245, loss: 0.630, elapsed:  4417 s\n",
      "....... Epoch: 246, loss: 0.630, elapsed:  4435 s\n",
      "....... Epoch: 247, loss: 0.630, elapsed:  4453 s\n",
      "....... Epoch: 248, loss: 0.630, elapsed:  4470 s\n",
      "....... Epoch: 249, loss: 0.629, elapsed:  4488 s\n",
      "....... Epoch: 250, loss: 0.630, elapsed:  4506 s\n",
      "....... Epoch: 251, loss: 0.629, elapsed:  4524 s\n",
      "....... Epoch: 252, loss: 0.630, elapsed:  4541 s\n",
      "....... Epoch: 253, loss: 0.629, elapsed:  4559 s\n",
      "....... Epoch: 254, loss: 0.629, elapsed:  4577 s\n",
      "....... Epoch: 255, loss: 0.629, elapsed:  4594 s\n",
      "....... Epoch: 256, loss: 0.629, elapsed:  4612 s\n",
      "....... Epoch: 257, loss: 0.629, elapsed:  4630 s\n",
      "....... Epoch: 258, loss: 0.629, elapsed:  4647 s\n",
      "....... Epoch: 259, loss: 0.629, elapsed:  4665 s\n",
      "....... Epoch: 260, loss: 0.629, elapsed:  4683 s\n",
      "....... Epoch: 261, loss: 0.629, elapsed:  4700 s\n",
      "....... Epoch: 262, loss: 0.629, elapsed:  4718 s\n",
      "....... Epoch: 263, loss: 0.629, elapsed:  4736 s\n",
      "....... Epoch: 264, loss: 0.628, elapsed:  4754 s\n",
      "....... Epoch: 265, loss: 0.629, elapsed:  4771 s\n",
      "....... Epoch: 266, loss: 0.629, elapsed:  4789 s\n",
      "....... Epoch: 267, loss: 0.629, elapsed:  4807 s\n",
      "....... Epoch: 268, loss: 0.628, elapsed:  4824 s\n",
      "....... Epoch: 269, loss: 0.628, elapsed:  4842 s\n",
      "....... Epoch: 270, loss: 0.628, elapsed:  4860 s\n",
      "....... Epoch: 271, loss: 0.629, elapsed:  4878 s\n",
      "....... Epoch: 272, loss: 0.628, elapsed:  4896 s\n",
      "....... Epoch: 273, loss: 0.628, elapsed:  4914 s\n",
      "....... Epoch: 274, loss: 0.629, elapsed:  4931 s\n",
      "....... Epoch: 275, loss: 0.629, elapsed:  4949 s\n",
      "....... Epoch: 276, loss: 0.628, elapsed:  4967 s\n",
      "....... Epoch: 277, loss: 0.629, elapsed:  4985 s\n",
      "....... Epoch: 278, loss: 0.628, elapsed:  5003 s\n",
      "....... Epoch: 279, loss: 0.628, elapsed:  5020 s\n",
      "....... Epoch: 280, loss: 0.628, elapsed:  5038 s\n",
      "....... Epoch: 281, loss: 0.628, elapsed:  5056 s\n",
      "....... Epoch: 282, loss: 0.628, elapsed:  5073 s\n",
      "....... Epoch: 283, loss: 0.628, elapsed:  5091 s\n",
      "....... Epoch: 284, loss: 0.628, elapsed:  5109 s\n",
      "....... Epoch: 285, loss: 0.627, elapsed:  5127 s\n",
      "....... Epoch: 286, loss: 0.627, elapsed:  5144 s\n",
      "....... Epoch: 287, loss: 0.628, elapsed:  5162 s\n",
      "....... Epoch: 288, loss: 0.628, elapsed:  5180 s\n",
      "....... Epoch: 289, loss: 0.627, elapsed:  5197 s\n",
      "....... Epoch: 290, loss: 0.627, elapsed:  5215 s\n",
      "....... Epoch: 291, loss: 0.628, elapsed:  5233 s\n",
      "....... Epoch: 292, loss: 0.628, elapsed:  5251 s\n",
      "....... Epoch: 293, loss: 0.627, elapsed:  5269 s\n",
      "....... Epoch: 294, loss: 0.627, elapsed:  5286 s\n",
      "....... Epoch: 295, loss: 0.628, elapsed:  5304 s\n",
      "....... Epoch: 296, loss: 0.627, elapsed:  5322 s\n",
      "....... Epoch: 297, loss: 0.627, elapsed:  5340 s\n",
      "....... Epoch: 298, loss: 0.627, elapsed:  5358 s\n",
      "....... Epoch: 299, loss: 0.627, elapsed:  5375 s\n",
      "....... Epoch: 300, loss: 0.627, elapsed:  5393 s\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# train\n",
    "#\n",
    "\n",
    "print(\"Starting NN training\")\n",
    "start = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    rows_left = len(df)\n",
    "    current = 0\n",
    "    chunk_num = math.ceil(rows_left/chunk_size)\n",
    "\n",
    "    #\n",
    "    # do every epoch in chunks\n",
    "    #\n",
    "    \n",
    "    model.zero_grad()\n",
    "    while rows_left > 0:\n",
    "        rows = (chunk_size if rows_left >= chunk_size else rows_left)\n",
    "        chunk = df[current:current+rows]\n",
    "        label_chunk = labels[current:current+rows]\n",
    "        \n",
    "        current += rows\n",
    "        rows_left -= rows\n",
    "        \n",
    "        print(\".\", end=\"\")\n",
    "        \n",
    "        # grab columns and labels, push into device (possibly GPU)\n",
    "        cols = [chunk[col].values for col in chunk.columns]\n",
    "        data = np.stack(cols, 1)\n",
    "        data = torch.tensor(data, dtype=torch.float).to(device)\n",
    "        output = torch.tensor(label_chunk).to(device)\n",
    "        \n",
    "        prediction = model(data)\n",
    "        single_loss = loss_function(prediction, output)\n",
    "        single_loss /= chunk_num\n",
    "        single_loss.backward()\n",
    "\n",
    "    # after done with all chunks, process the step and reset the gradients\n",
    "    optimizer.step()\n",
    "    model.zero_grad()\n",
    "      \n",
    "    print(f' Epoch: {i+1:3}, loss: {single_loss.item()*chunk_num:1.3f},', end=' ')\n",
    "    print(f'elapsed: {time.time()-start:5.0f} s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN training complete. Minutes elapsed: 90\n"
     ]
    }
   ],
   "source": [
    "print(f\"NN training complete. Minutes elapsed: {(time.time()-start)/60:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (batch_norm_num): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=150, out_features=180, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=180, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.4, inplace=False)\n",
       "    (12): Linear(in_features=50, out_features=20, bias=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Dropout(p=0.4, inplace=False)\n",
       "    (16): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Phase two - model evaluation\n",
    "\n",
    "model.eval() # switch into eval mode\n",
    "model.cpu() # get model back into CPU space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: 1338222 rows, 150 columns\n"
     ]
    }
   ],
   "source": [
    "# Import dev DF from CSV\n",
    "\n",
    "if debug == True:\n",
    "    dataset = 'data/mini_dev_encoded.csv'\n",
    "else:\n",
    "    dataset = 'data/dev_encoded.csv'\n",
    "\n",
    "test = pd.read_csv(dataset)\n",
    "\n",
    "if \"HasDetections\" in test.columns:\n",
    "    # Remove hasdetections from the df.  Make it a new df for labels\n",
    "    test_labels = test['HasDetections'].to_numpy()\n",
    "    test.drop(columns=['HasDetections'], inplace=True)\n",
    "\n",
    "# Iterate and convert all data values\n",
    "\n",
    "cols = []\n",
    "for c in test.columns:\n",
    "    test[c] = test[c].astype(np.float64) # Convert dtypes to be all identical for pytorch\n",
    "\n",
    "print(f\"Test data: {len(test)} rows, {len(test.columns)} columns\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1338222, 150])\n",
      "torch.Size([1338222, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# grab columns and labels, push into device (possibly GPU)\n",
    "cols = [test[col].values for col in test.columns]\n",
    "data = np.stack(cols, 1)\n",
    "data = torch.tensor(data, dtype=torch.float)\n",
    "print(data.shape)\n",
    "\n",
    "output = model(data)\n",
    "print(output.shape)\n",
    "predictions = np.argmax(output.cpu().data.numpy(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1338222,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1338222,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6480158000690468\n",
      "[1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1]\n",
      "[0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=test_labels, y_pred=predictions))\n",
    "print(predictions[:30])\n",
    "print(labels[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
